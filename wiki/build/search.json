[





{
"title": "Caterpillar With Hydro",
"tags": "hydrodynamicsgasstatus",
"keywords": "",
"url": "doc_caterpillar_hydro.html",
"summary": "Here you can find the status of the gas runs of the Caterpillar suite.",
"body": "Caterpillar With HydroTo be added soon."
},




{
"title": "Consistent-Trees",
"tags": "treescataloguesmergerhalo",
"keywords": "",
"url": "doc_consistenttrees.html",
"summary": "The halos from rockstar are connected together in time using consistent trees. Here you can find information about the structure of the tree and various quantities relate to the formation history of a given halo or subhalo(s).",
"body": "OverviewThe code&#39;s paper can be found here. Documentation on getting it running and a more exhaustive documentation of the catalogue&#39;s content at its Bitbucket Repository.Catalogue ParametersThe consistent-trees output it similar to that of the rockstar catalogue except now there are additional paramters which connect the halos in time.variableunitsdefinitionid-id of the halo in the merger treepid-parent idupid-bfid-breadth first order iddfid-depth first order idorigid-original id as identified in Rockstar cataloguelastprog_dfid-last progenitor depth first idsnap-snapshotscale-scale factordesc_id-descendant idnum_prog-number of progenitorsphantom-whether it is a phantom halosam_mvirMsol/hsmoothed virial massmvirMsol/hvirial mass (Brian &amp; Norman prescription) (it is actually mgrav from Rockstar)rvirkpcvirial radius (Brian &amp; Norman prescription)rskpcscale radiusvrmskm/srms velocitymmp-most massive progenitorscale_of_last_MM-scale factor of last major mergervmaxkm/smaximum circular velocityposXkpcx-positionposYkpcy-positionposZkpcz-positionpecVXkm/sx-dir peculiar velocitypecVYkm/sy-dir peculiar velocitypecVZkm/sz-dir peculiar velocityJxx-component angular momentumJyy-component angular momentumJzz-component angular momentumspinspin parameter (Peebles version)spin_bullockspin parameter (Bullock modified version)m200c_allMsol/hvirial mass of all particles based on 200x critical densitym200bMsol/hvirial mass of all particles based on 200x background densityxoffkpcPosition offset parameter abs(center of mass position - halo position)voffkpcVelocity offset parameter abs(center of mass velocity - halo velocity)b_to_a(500c)-Halo ellipticity b/ac_to_a(500c)-Halo ellipticity c/aA[x](500c)x-axis for ellipticityA[y](500c)y-xis for ellipticityA[z](500c)z-xis for ellipticityT/abs(U)-Virial ratio (kinetic/abs(potential))"
},




{
"title": "Data Access Examples",
"tags": "examplespythoncodeaccess",
"keywords": "",
"url": "doc_data_access_examples.html",
"summary": "Here are some examples to get you started working with the Caterpillar data.",
"body": "OverviewHere you&#39;ll find all sorts of information about analysing the Caterpillar halos. If you still aren&#39;t sure what to do after reading the documentation please send an email to Brendan Griffen. Make sure you have setup your work environment correctly and obtained the necessary libraries.Examples (under construction)One example combining these two modules with some general inspection would look something like as follows:import brendanlib.grifflib as glibimport haloutils as htils# load the first 24 halos# (just change 14 to 11 for the lower resolution halos)hpaths = htils.get_paper_paths_lx(14)# loop through all high-resolution halosfor hpath in hpaths:    # strip down the path to just the halo id    parentid = htils.get_parent_hid(hpath)    # get the central host id of the zoom-in halo    # parentid is named from the parent simulation     # the zooms have different ids    zoomid = htils.load_zoomid(hpath)    # Return the pandas data frame with all the halos,     # return mvir for example    lastsnap = htils.get_lastsnap(hpath)    halos = htils.load_rscat(hpath,lastsnap,verbose=True)    mvir_host = halos.ix[zoomid][&#39;mvir&#39;] # units of Msol/h    # get one specific parameter of the host at    # z = 0 (quick version of above)    mvir_host = htils.get_quant_zoom(hpath,&#39;mvir&#39;) # units of Msol/h    # return all halo virial masses    all_mvir = halos[&#39;mvir&#39;] # units of Msol/h    # get the merger tree of the host and all its subs    cat = htils.load_mtc(hpath,haloids=[zoomid])    # read in every halo&#39;s merger tree    all_trees = htils.load_mtc(hpath,indexbyrsid=True)    tree = all_trees[zoomid]     # if you feed more than one id to the above     # it would be cat[0],cat[1] etc.    # you can now access the main branch &gt; try tree. then tab complete to see other functions    mainbranch = tree.getMainBranch()     # we also have a short hand version:    # mainbranch = htils.get_mainbranch(hpath)     # which is very fast and skips the reading of the entire progenitor tree if you don&#39;t need dit    # see further down this page for more options    # output the main branch virial mass    print mainbranch[&#39;mvir&#39;] Functions Available Once You&#39;ve Loaded A Rockstar CatalogueTo load a rockstar catalogue for a given snapshot, you simple do:rscat = htils.load_rscat(hpath,snapshot,verbose=True)Once you do this you will have the ability to do the following kinds of tasks (rscat. then tab complete): def __init__(self, dir, snap_num, version=2, sort_by=&#39;mvir&#39;, base=&#39;halos_&#39;, digits=2, AllParticles=False):   def get_particles_from_halo(self, haloID):       # @param haloID: id number of halo. Not its row position in matrix       # @return: a list of particle IDs in the Halo   def get_subhalos_from_halo(self,haloID):       #Retrieve subhalos only one level deep.       #Does not get sub-sub halos, etc.   def get_subhalos_from_halos(self,haloIDs):       #Returns an array of pandas data frames of subhalos. one data frame       #for each host halo. returns only first level of subhalos.   def get_subhalos_from_halos_flat(self,haloIDs):       #Returns a flattened pandas data frame of all subhalos within       #the hosts given by haloIDs. Returns only first level of subhalos.   def get_hosts(self):       # Get host halo frame only   def get_subs(self):       # Get subhalo frame only   def get_all_subs_recurse(self,haloID):       # Retrieve all subhalos: sub and sub-sub, etc.        # just need mask of all subhalos, then return data frame subset   def get_all_subhalos_from_halo(self,haloID):       # Retrieve all subhalos: sub and sub-sub, etc.       # return pandas data frame of subhalos   def get_all_sub_particles_from_halo(self,haloID):       #returns int array of particle IDs belonging to all substructure       #within host of haloID   def get_all_particles_from_halo(self,haloID):       #returns int array of all particles belonging to haloID   def get_all_num_particles_from_halo(self,haloID):       # Get the actual number of particles &#39;total_npart&#39; from halo as opposed to &#39;npart&#39;.       # mainly for versions less than 7   def get_block_from_halo(self, snapshot_dir, haloID, blockname, allparticles=True):       # quick load a block (hdf5 block) of particles belong to halo.       # e.g. you want particle positions for haloid = 10 (use blockname=&quot;pos&quot;)       # this works fastest on snapshots ordered by id and requires import readsnapHDF5_greg    def H(self):       #returns hubble parameter for rockstar run   def get_most_gravbound_particles_from_halo(self,snapshot_dir, haloID):       # Gets most bound particles just based on potential energy for specific halo ID   def get_most_bound_particles_from_halo(self, snapshot_dir, haloID):       # Gets most bound particles for halo based on pot. energy and kin. energy       # if potential block does not exist, it is calculate assuming a spherical halo   def getversion(self):       # returns the version of rockstar the run was done within       # this will include versions made by Alex Ji, Greg Dooley &amp; Brendan GriffenHere is an example of some of these in action:# load required modulesimport haloutils as htils import numpy as np# select the caterpillar halo of interestwant_cat_sim = 5# select the last snapshot (z = 0)snapshot = htils.get_lastsnap(hpath)# load rockstar id of the host halozoomid = htils.load_zoomid(hpath)# get the directory path for this halohpaths = htils.get_paper_paths_lx(14)hpath = hpaths[want_cat_sim-1]#Load Haloshalos = htils.load_rscat(hpath,snapshot)#Select host haloshosts = halos.get_hosts()#Select subhalossubs = halos.get_subs()# Get positions of subs and hostsprint np.array(hosts.posX),np.array(hosts.posY),np.array(hosts.posZ) print np.array(subs.posX),np.array(subs.posY),np.array(subs.posZ) #Get particle ids from halo of interest (here it is the host)print halos.get_particles_from_halo(zoomid) #Get virial radius of a specific halo idprint halos.ix[zoomid][&#39;rvir&#39;] # units of kpc/hMerger Tree ManipulationSimilarly the merger tree catalogues (once loaded) have a number of its own functions.def getMainBranch(self, row=0):   &quot;&quot;&quot;   @param row: row of the halo you want the main branch for. Defaults to row 0   Uses getSubTree, then finds the smallest dfid that has no progenitors   @return: all halos that are in the main branch of the halo specified by row (in a np structured array)   &quot;&quot;&quot;def getMMP(self, row):   &quot;&quot;&quot;   @param row: row number (int) of halo considered   @return: row number of the most massive parent, or None if no parent   &quot;&quot;&quot;def getNonMMPprogenitors(self,row):   &quot;&quot;&quot;   return row index of all progenitors that are not the most massive   These are all the subhalos that were destroyed in the interval   &quot;&quot;&quot;These can be used in the following example:# load required modulesimport haloutils as htils import numpy as np# select the caterpillar halo of interestwant_cat_sim = 5# select the last snapshot (z = 0)snapshot = htils.get_lastsnap(hpath)# load rockstar id of the host halozoomid = htils.load_zoomid(hpath)# Load every tree (VERY slow)trees = htils.load_mtc(hpath)# Just look at the first tree# tree = cat[0]# You can access all trees via: cat[0], cat[1] etc.# If you want to load the tree for a particular ID from the rockstar cataloguetrees = htils.load_mtc(haloids=[zoomid]) # in this case the host (quite slow) tree = cat[0] # tree will contain all progenitors, including subhalos# Just say you want to index the merger tree by the z = 0 root rockstar id # (i.e. the base of the tree). This is quite powerful because you might select# halos of interest in the rockstar catalogue then want to know, just for those# what their merger tree is (e.g. say you just want dwarf systems of a particular size)trees = htils.load_mtc(haloids=[zoomid],indexbyrsid=True) tree = trees[zoomid]# You can just loop through rockstar ids (if you gave it more than one id above)# and get out the accretion histories for a small sample of trees quite quickly# to get the main branchmain_branch = tree.getMainBranch()# print mass evolutionfor mass,scale in zip(main_branch[&#39;mvir&#39;],main_branch[&#39;scale&#39;]):   print &quot;%3.2f: %3.2e&quot; % (scale,mass)# output1.00: 2.86e+14 0.99: 2.89e+14 0.98: 2.90e+14 0.97: 2.90e+14 0.95: 2.88e+14 0.94: 2.86e+14 0.93: 2.83e+14 0.92: 2.80e+14 0.91: 2.76e+14 0.90: 2.73e+14 0.89: 2.64e+14 0.88: 2.47e+14 ... Accessing Particle DataIf you want the Gadget header:header = htils.get_halo_header(hpath)``pythonget a random halohpaths = htils.get_paper_paths_lx(14)hpath = hpaths[0]header = htils.get_halo_header(hpath)header.(tab complet)header.boxsize        header.massarr        header.omegaLheader.cooling        header.metals         header.redshiftheader.double         header.nall           header.sfrheader.feedback       header.nall_highword  header.stellar_ageheader.filenum        header.npart          header.timeheader.hubble         header.omega0 ```See the Gadget section in the sidebar for more information on the header and block types available.If you wanted to get the postions of all the particles for a specific halo (or any block). pos halos.load_partblock(hpath,zoomid,&quot;POS &quot;) # units Mpc/h# &quot;VEL &quot;, &quot;ID  &quot; also work (notice the space)print pos*1000. # kpc/h# output[ 32085.45117188  57312.79296875  44314.35546875][ 27002.18554688  10062.73242188   9899.70019531][ 26711.08789062   9560.22460938  10165.18847656][ 49757.3515625   21470.00195312   6461.90917969]...If you wanted just the ids for a selection of particle idspos halos.load_partblock(hpath,zoomid,&quot;POS &quot;,partids=[listofids]) # units Mpc/h"
},




{
"title": "Datasets",
"tags": "postprocesseddatasets",
"keywords": "",
"url": "doc_datasets.html",
"summary": "Here you can find a range of available datasets which have been postprocessed from the Caterpillar simulation suite.",
"body": "Available DatasetsTo be added soon."
},




{
"title": "Frequently Asked Questions",
"tags": "faq",
"keywords": "frequently asked questions, FAQ, question and answer, collapsible sections, expand, collapse",
"url": "doc_faq.html",
"summary": "Here you can find some frequently asked questions of the collaboration.",
"body": "                                                                                                                        When will Caterpillar be finished?                                                                                                                                        This depends on the availability of computing time and the limitations of hardware. At the moment we are running many simulations at the same time. This only occurs in bursts when we have available time and we personally have time to oversee them. Each simulation takes roughly 300k hours (exluding halo finding ~2 weeks + postprocessing which is ~1-3 days). With all of these time heavy tasks, we still estimate to have the full sample by Spring/Summer of 2016.                                                                                                                                                                                                    What is the authorship policy?                                                                                                                                        Since the collaboration is quite small there is a lot of room to grow with new science. That being said, there are a few core projects which the collaboration is working towards (see the on-going projects page). Provided there is no overlap with the core science projects of the collaboration the data is freely available for anyone who would like to use it. Naturally we hope you collaborate with people within the group so you can achieve your goals as soon as possible as many of us have had years creating/analyzing the data.                                                                                                                                                                                                    How is the data going to be released to the public?                                                                                                                                        At the moment, we don't have a full plan in place for complete open access to the data so for the moment individuals are being given access an analysis node at MIT (bigbang). Through this portal, the entire suite is available. The size of the data (300TB+ and growing) makes it difficult to find a one size fits all solution to the public release. We are working with Illustris folks at Harvard to see if there are some optimum ways to deal this kind of load.                                                                                                                                                                                                    Will the Caterpillar halos be run with gas/hydro?                                                                                                                                        The short answer is yes, eventually. We have already setup and run gas runs of the parent simulation and are in the process of running hydro runs for the zoom-in simulations. There are many more pieces to this next stage of the project and they are still being interally reviwed (i.e. sure hydro, but _which_ flavour?). This wiki will update the status of the hydro runs in due course.                                                                                                                                                                                                    Can I use the tools/code for my own projects?                                                                                                                                        The code is freely available to the community though if it is a non-trivial tool, we ask if you keep the developers (Alex, Greg and Brendan) in the loop so you done become unstuck using the code for the wrong reasons. Similarly, we may be able to help you get a given tool tailored for your own purposes so please let us know if you would like any assistance.                                                                                                                                                                                                    When will you have something us observers can sink our teeth into?                                                                                                                                        Yes, Caterpillar is a dark matter only simulation. Though, that being said quite a lot can be learned from this simulation suite alone. Our main goal however is to run sophisticated semi-analytic models over the merger trees to generate mock galaxies (i.e. with stellar masses, cold gas fractions, morphologies etc.). The Munich SAM is currently being run over the Caterpillar halos and will produce a fantastic data set to do all sorts of analysis (e.g. assembly of the stellar halo, chemical tagging, stellar streams).                                                                                                                                                      "
},




{
"title": "Gadget",
"tags": "analysisdata",
"keywords": "",
"url": "doc_gadget.html",
"summary": "Here you can find information relating to our core running code; Gadget.",
"body": "ParametersFor the simulation suite we use P-Gadget3 (first 24 halos) and Gadget4 (remainder). The parent directory of each folder contains .gadget3 or .gadget4 to indicate which halo was run with which code. We use the following compile flags for our runs. PMGRID changes depending on the resolution of the run. There is a great amount of documentation on each of these parameters on Phil Hopkin&#39;s GIZMO website.PERIODIC  UNEQUALSOFTENINGS  PMGRID=512  GRIDBOOST=2  PLACEHIGHRESREGION=2  ENLARGEREGION=1.2  MULTIPLEDOMAINS=16  PEANOHILBERT  WALLCLOCK  MYSORT  DOUBLEPRECISION  OUTPUT_IN_DOUBLEPRECISION  INPUT_IN_DOUBLEPRECISION  ORDER_SNAPSHOTS_BY_ID  NO_ISEND_IRECV_IN_DOMAIN  FIX_PATHSCALE_MPI_STATUS_IGNORE_BUG  COMPUTE_POTENTIAL_ENERGY  OUTPUTPOTENTIAL  RECOMPUTE_POTENTIAL_ON_OUTPUT  HAVE_HDF5  DEBUG  Parameter files for each of the runs can be found as param.txt in the relevant simulation directory. For those users who have access to antares.mit.edu (MKI&#39;s compute cluster), Brendan Griffen&#39;s home directory contains the master files (configuration files, parameter files and expansion factor lists for output) for the entire suite./home/bgriffen/exec/gadget3/home/bgriffen/exec/gadget4"
},




{
"title": "What Is The Caterpillar Project?",
"tags": "historyoverviewmotivationscience",
"keywords": "",
"url": "doc_history_motivation.html",
"summary": "To get started with the Caterpillar project, first make sure you have a firm understanding of the strengths and weaknesses of the data.",
"body": "A Brief HistoryThe Caterpillar project was the brainchild of Prof. Anna Frebel and Lars Hernquist at a meeting at the CfA in 2013. Phillip Zukin was then a PhD student under Edmund Bertschinger who was temporarily employed to carry out the first parent volume simulations that would become the Caterpillar project. His work continued until Brendan Griffen arrived as the new postdoc to take charge of the project in its entirety. Graduate students Greg Dooley and Alex Ji are also working part time on the project. At a conference at UC Irvine in early 2014 between Anna Frebel, Brian O&#39;Shea, Brendan Griffen and Facundo Gomez, it was agreed that we would combined datasets and future efforts (Brian and Facundo had a similar sized project for studying stellar halos at the time). In September of 2015, the first Caterpillar paper was put on arxiv.MotivationThe Caterpillar project is a large simulation suite of dark matter halos of mass similar in size to that of the Milky Way. By virtue of the extremely large number of halos, temporal and spatial resolution of the simulations, it is the largest simulation suite of its kind in the world. We plan to extend this initial release sample to 70 halos in total. Many of these 70 halos have already completed but are currently unpublished. All halos are accessible to the community by contacting the collaboration.The goal of the Caterpillar Project is to use these relatively inexpensive dark matter simulations to:statistically probe the merger history of Milky Way-like galaxies,understand the origin and evolution of the satellite systems of Milky Way-like galaxies anddetermine to what extent the Milky Way is unusual relative to its similar sized cousins.Summary of Method: Tip:  Much more detail of the Caterpillar project can be found in Griffen et al. (2015).The Caterpillar suite was run using P-Gadget3 and Gadget4, tree-based n-body codes based on Gadget2 (Springel et al. 2005). For the underlying cosmological model we adopt the \\(\\Lambda\\)CDM parameter set characterised by a Planck cosmology given by, \\(\\Omega_m=0.32\\), \\(\\Omega_\\Lambda=0.68\\), \\(\\Omega_b=0.05\\), \\(n_s=0.96\\), \\(\\sigma_8=0.83\\) and Hubble constant, H = 100 \\(h\\ km\\ s^{-1}\\ Mpc^{-1}\\) = 67.11 \\(km\\ s^{-1}\\ Mpc^{-1}\\) (Planck et al. 2014). All initial conditions were constructed using music (Hahn &amp; Able 2011).  We identify dark matter halos via Rockstar (Behroozi et al. 2013) and construct merger trees using consistent-trees (Behroozi et al. 2012). Rockstar assigns virial masses to halos, \\(M_{vir}\\), using the evolution of the virial relation from Bryan &amp; Norman (1998). for our particular cosmology. At z = 0, this definition corresponds to an over-density of 97x the critical density of the Universe. We have modified rockstar to output all particles belonging to each halo so we can reconstruct any halo property in post-processing if required. We have also improved the code to include iterative unbinding. In our work, we restrict our definition of virial mass to include only those particles which are bound to the halo."
},




{
"title": "Initial Conditions",
"tags": "",
"keywords": "ics, initial conditions, parameter file, music",
"url": "doc_initial_conditions.html",
"summary": "Overview of the initial conditions used for the Caterpillar suite.",
"body": "OverviewWe use the multi-scale cosmological initial conditions creator MUSIC. MUSIC is a computer program to generate nested grid initial conditions for high-resolution &quot;zoom&quot; cosmological simulations. A detailed description of the algorithms can be found in Hahn &amp; Abel (2011). You can download the user&#39;s guide here or obtain a copy of the code here. Any questions should be directed to Brendan Griffen and then if that fails, Oliver Hahn.ParametersThese are the parameter files which were used to generate the initial coditions for the Caterpillar project. We adopt the raw Planck (2013) cosmology and do not use 2nd order lagrangian perturbation theory. The user guide contains all the information required to understand the following parameter files. Within each simulation directory, there is a file named OUTPUTmusic which gives the log of the construction of the initial conditions.Parent SimulationFirst we constructed a parent simulation of sufficient size to include thousands of Milky Way sized systems but also of sufficient resolution to construct good lagrangian volumes. We decided on ~10,000 particles per Milky Way-sized host and a 100 Mpc/h volume. Our level_max is 10 below, which means our parent simulation&#39;s effective resolution is (210)3 = (1024)3 (i.e. a particle mass of 8.72 x 107 \\(M_\\odot/h\\).If you have access to the MIT cluster (bigbang.mit.edu), you can access the parent simulation data here: /bigbang/data/AnnaGroup/caterpillar/parent/gL100X10/. Within this folder you&#39;ll find the following file (within ics/): # parentics.conf[setup]boxlength               = 100zstart                  = 127levelmin                = 10levelmin_TF             = 10levelmax                = 10padding                 = 8overlap                 = 4ref_center              = 0.5, 0.5, 0.5ref_extent              = 0.2, 0.2, 0.2align_top               = yesbaryons                 = nouse_2LPT                = nouse_LLA                 = noperiodic_TF             = yes[cosmology]Omega_m                 = 0.3175      Omega_L                 = 0.6825       Omega_b                 = 0.049     H0                      = 67.11         sigma_8                 = 0.8344        nspec                   = 0.9624        transfer                = eisenstein[random]seed[10]                = 34567[output]format                  = gadget2filename                = ./icsgadget_num_files        = 128[poisson]fft_fine                = yesaccuracy                = 1e-5pre_smooth              = 3post_smooth             = 3smoother                = gslaplace_order           = 6grad_order              = 6Zoom SimulationsEach simulation folder has a configuration file for MUSIC (e.g. H1079897_EX_Z127_P7_LN7_LX14_O4_NV4.conf) which was used to construct the initial conditions.There are a few things to note about the zoom-in paramter files when compared to the parent volume parameter file. First is we now specify a region_point_file which defines the x,y,z (normalized to the box width) of the particles to be resampled. We have added the parameter hipadding which our own modification to allow for expanded regions. 1.05, for example, represents an expanded ellipsoid (by 5%). See Section 2.3 of Griffen et al. (2015) for a more detailed description of these geometries and their impact on contamination.We also draw the reader&#39;s attention to the seed values. Note that we do not set the seed values for any level lower than the parent volume (10) which makes MUSIC smooth out any levels lower than 10. The seed values at 11 are simply the halo numbers and then each level higher scales by a factor of 2 of this original number. This was required because the ICs were generated through our automatic pipeline and each simulation needs unique values to seed the random noise field. The levelmin_TF is also set to be the same as the parent volume (10). The padding and overlap parameters are the same for all simulations.# H1079897_EX_Z127_P7_LN7_LX14_O4_NV4.conf[setup]boxlength            = 100zstart               = 127levelmin             = 7levelmin_TF          = 10levelmax             = 14padding              = 7overlap              = 4region               = ellipsoidhipadding            = 1.05region_point_file    = /n/home01/bgriffen/data/caterpillar/ics/lagr/H1079897NRVIR4align_top            = nobaryons              = nouse_2LPT             = nouse_2LLA             = noperiodic_TF          = yes[cosmology]Omega_m              = 0.3175Omega_L              = 0.6825Omega_b              = 0.049H0                   = 67.11sigma_8              = 0.8344nspec                = 0.9624transfer             = eisenstein[random]seed[10]              = 34567seed[11]              = 1079897seed[12]              = 2159794seed[13]              = 3239691seed[14]              = 4319588[output]format               = gadget2_doublefilename             = ./icsgadget_num_files     = 8gadget_spreadcoarse  = yes[poisson]fft_fine             = yesaccuracy             = 1e-05pre_smooth           = 3post_smooth          = 3smoother             = gslaplace_order        = 6grad_order           = 6"
},




{
"title": "Join Collaboration",
"tags": "postprocesseddatasets",
"keywords": "",
"url": "doc_join_collaboration.html",
"summary": "Here you can find a range of available datasets which have been postprocessed from the Caterpillar simulation suite.",
"body": "Available DatasetsTo be added soon."
},




{
"title": "MIT Compute Cluster",
"tags": "clustercomputinghardwareslurmmitamd",
"keywords": "",
"url": "doc_mit_cluster.html",
"summary": "Information relating to the compute cluster at MKI.",
"body": "The MKI cluster (with respect to Caterpillar) at MIT has one login node (antares.mit.edu) which connects to a three primary blocks of slave nodes and one analysis node (bigbang.mit.edu). Compute Nodes (for farming jobs)RegNodesBlock 1:* 22 nodes (176 cores)* 8 core [2 Ghz]* 16GB memory* 1 Gbit interconnectBlock 2:* 9 nodes (72 cores)* 8 cores, 2.5 Ghz* 16GB memory* 1 Gbit interconnectThis partition should be used for serial jobs. All nodes can be accessed via using the partition RegNodes.HyperNodes12 nodes (144 cores)12 cores, 2.3 Ghz (24 with hyper-threading)24GB memory10 Gbit interconnectThis partition should only be used for MPI enabled codes as it has faster interconnect.AMD648 nodes, 512 cores64 cores, 2.2 Ghz256GB memory10 Gbit interconnectThis partition should only be used for extremely expensive runs which require 100GB++ memory and for codes that are MPI enabled. You can also used openMP multi-threaded jobs which require large amounts of memory (1 job per node). At this stage, there are no time-limits or fixed memory limits for each of the nodes.Analysis Node (for plotting, running scripts etc.)The Frebel group as MIT also has access to an analysis node which is connected to the Caterpillar data called bigbang. It can be accessed via ssh username@bigbang.mit.edu. See Data Access on this site to get started with your analysis once you understand the storage layout below.Storage File Systems/ServersThe Frebel group has access to ~120GB RAID storage and 300TB+ of ZFS tape storage. The ZFS tape storage consists of two 130TB servers (grinder and cruncher). These are significantly slower to do I/O from the first time. They each have solid state drives on them acting as a large cache which makes reading data comparable to the RAID drives if you are repeatedly accessing the same snapshot on the tape, for example. The directory structure is as follows:login node: ssh username@bigbang.mit.edu# ZFS TAPE STORAGE SERVERS (not as fast as the RAID  slow to load) -&gt; cruncher/  -&gt; caterpillar/   -&gt; parent/   -&gt; halos/    -&gt; H1079897/ -&gt; grinder/  # ZFS tape drive with   -&gt; caterpillar/   -&gt; halos/    -&gt; H1387186    ...  -&gt; aquarius/   -&gt; Aq-A/   -&gt; Aq-B/   -&gt; Aq-C/   -&gt; Aq-D/   -&gt; Aq-E/   -&gt; Aq-F/# RAID STORAGE -&gt; data/AnnaGroup/     # RAID drive with all runs lower than LX14  -&gt; caterpillar/   -&gt; halos/            # master folder with all of the caterpillar data within    -&gt; H1079897/        # halos are named according to their folders    ...   -&gt; ics/    -&gt; lagr/            # lagrangian volume files   -&gt; parent/           # parent simulation runs    -&gt; gL100X10/        # actual parent simulation (X10 refers to Level_max=10 in MUSIC)Current Storage StatusWe are quite limited with storage space and so we ask you to request more storage if you need it. You will then be allocated a directory on bigbang/data/{username}/. The current break down of the storage capability is as follows:bigbang% df -hFilesystem            Size  Used Avail Use% Mounted on/dev/mapper/vg_bigbang-lv_home                       57G  1.9G   52G   4% /home/dev/mapper/vg_bigdata-lv_bigdata                      121T   93T   28T  78% /bigbang/data/cruncher/data                      127T  114T   14T  90% /cruncher/data/grinder/data                      127T   74T   54T  59% /grinder/data/spacebase/data                       36T   33M   36T   1% /spacebase/dataWhere is the data actually stored?Due to our storage limitations we have opted for the following strategyyou should only ever get your hpaths from haloutils via hpaths = haloutils.get_paper_paths_lx(14), for example. Do not get your halo paths from ever going into caterpillar/halos/middle_mass_halos/*.all simulation runs lower than LX14 (i.e. LX11,12,13) have been stored on the RAID drives.all LX14 runs are symbolically linked on the RAID drives but might not actually exist physically on the drives. all halo catalogues are on the RAID drives so access for these is very fastall particle data except the last snapshot are actually stored on the ZFS tape drives (though it will appear in outputs/ as a symlink)only the last snapshot of the particle data is actually stored on the RAID drivesall postprocessed catalogues in analysis/ of each halo directory are on the RAID drivesall of the aquarius data is on the ZFS tape storage as this is not often usedthe parent simulation (gL100X10) is stored in the same way as the LX14 runs: only the last snapshot of the particle data is stored on the RAID drives, everything else is on the ZFS tape drives."
},




{
"title": "Commenting on files",
"tags": "modulesanalysiscodepythondata",
"keywords": "",
"url": "doc_modules_and_analysis.html",
"summary": "Once you have an account on the MKI compute cluster you can use these tools to get your started on accessing the Caterpillar data.",
"body": "Setting Up Your Work Environment &amp; ToolsRead the cluster information.Keep in mind: bigbang.mit.edu (our group box) is where you want to do your analysis, access data etc. and antares.mit.edu is the login node for the cluster for the whole Institute. Remember: analysis = bigbang, jobs = antares! See Compute Cluster Information in the sidebar for more detail on these two machines and how the data is actually distributed.Get our modules from the Caterpillar Pproject github page. These include the following two critical libraries:git@github.com:caterpillarproject/analysis.gitgit@github.com:caterpillarproject/modules.gitYou can obtain them in the following way (ensure you are in the directory you want them to be placed):&gt; git clone git@github.com:caterpillarproject/analysis.git&gt; git clone git@github.com:caterpillarproject/modules.git(sorry we dont have any support for IDL or Matlab at this time - please share any you develop!)Add the following to your .cshrc file.setenv PYTHONPATH /path/to/modules:$PYTHONPATHsetenv PYTHONPATH /path/to/analysis:$PYTHONPATHInstall the Anaconda python distribution and be sure to run conda update periodically. You can alsoYou will also need asciitable, h5py and pandas to make full use of our module package. You can install some of these packages with Anaconda by &gt; conda install pandas, for example. If that fails, simply do &gt; pip install pandas etc. to install each of these. When you open ipython, you should be able to import each of these - please make sure you can before importing the modules! You&#39;re now ready to roll!"
},




{
"title": "Caterpillar Parent Simulation",
"tags": "parentsimulationzoom",
"keywords": "",
"url": "doc_parent_simulation.html",
"summary": "Information about the creation and running of the parent simulation as well as the selection method used for the Caterpillar halos.",
"body": "The parent simulation was setup primarily to find Milky Way-sized candidates at z = 0. All the parameters used for the construction of the simulation were oriented around this goal.OverviewVolume  h^3 Mpc^3# particles\\(m_{dm}\\)  10^7 h^-1 \\(M_\\odot\\)\\(m_{dm}\\)  10^7 \\(M_\\odot\\)\\(\\epsilon_{dm}\\)  pc100^31024^39122441 Note:  The parent simulation initial conditions were run on an older version of MUSIC.VolumeThe volume of the parent simulation was selected to be 100 Mpc/h as this allows for roughly ~6500 Milky Way-sized (i.e. 10^12 Msol) systems to be found. After a gentle selection over local environment (i.e. making sure no halos were near clusters) 2122 candidates were used to select Caterpillar candidates.Mass ResolutionWe required a resolution which allowed us to resolve 10^12 Msol halos with 10,000 particles so as to construct well defined lagrangian volumes. This resulted in us selecting a resolution of 10243 or a particle mass of 8.72 x 107 \\(M_\\odot/h\\).Halo SelectionWe selected halos with the following environmental requirements:halos between 0.7 - 3 x 1012 \\(M_\\odot\\) (6564 candidates)no halos larger than 7 x 1013 \\(M_\\odot\\) within 7 Mpcno halos larger than 7 x 1012 \\(M_\\odot\\) within 2.8 Mpc (2122 candidates)This is roughly in line with Tollerud et al. (2012), Boylan-Kolchin et al. (2013), Fardal et al. (2013), Pfiffel et al. (2013), Li &amp; White (2008), van der Marel et al. (2012), Karachentsev et al. (2004) and Tikhonov &amp; Klypin (2009). This avoids Milky Way-sized systems near clusters but does not make them overly isolated necessarily. Halos were also selected to not be preferentially near the very edge of the simulation volume as a matter of convenience. The first 24 Caterpillar halos are highlighted within the parent volume below.Temporal ResolutionThe time steps were set to be log of the expansion factor, following a similar convention to that used by the Millenium and Millenium-II simulations. The following table shows the various measures for time/size at each snapshot. Tip:  Be sure to use the halo utility module (haloutils) in Python for quickly getting the temporal quantity for a given snapshot. See data access for more information.  Download CSVSnapScale FactorRedshiftTime00.021346.00000.053510.029033.50290.085120.036726.25570.121230.044421.52450.161340.052118.19290.205150.059815.71990.252260.067513.81140.302570.075212.29400.355780.082911.05860.411790.090610.03330.4704100.09839.16870.5316110.10608.42970.5952120.11387.79090.6612130.12157.23310.7294140.12926.74190.7998150.13696.30600.8723160.14465.91660.9469170.15235.56661.0234180.16005.25031.1018190.16774.96301.1821200.17544.70111.2642210.18314.46111.3481220.19084.24061.4337230.19854.03711.5210240.20623.84891.6098250.21393.67421.7003260.22163.51171.7923270.22943.36011.8858280.23713.21841.9808290.24483.08562.0772300.25252.96082.1749310.26022.84352.2741320.26792.73302.3745330.27562.62862.4762340.28332.52992.5792350.29102.43642.6834360.29872.34772.7888370.30642.26352.8953380.31412.18353.0029390.32182.10723.1116400.32952.03463.2213410.33721.96523.3321420.34491.89903.4438430.35271.83563.5565440.36041.77503.6701450.36811.71693.7846460.37581.66123.9000470.38351.60774.0161480.39121.55634.1331490.39891.50694.2508500.40661.45944.3693510.41431.41374.4884520.42201.36964.6082530.42971.32714.7287540.43741.28614.8497550.44511.24654.9714560.45281.20835.0936570.46051.17135.2163580.46831.13565.3395590.47601.10105.4632600.48371.06755.5873610.49141.03515.7118620.49911.00375.8367630.50680.97325.9620640.51450.94376.0876650.52220.91506.2135660.52990.88716.3396670.53760.86016.4660680.54530.83386.5927690.55300.80826.7195700.56070.78346.8465710.56840.75926.9737720.57610.73577.1010730.58380.71287.2284740.59160.69057.3559750.59930.66877.4835760.60700.64757.6111770.61470.62697.7387780.62240.60677.8663790.63010.58717.9939800.63780.56798.1215810.64550.54928.2490820.65320.53098.3764830.66090.51318.5038840.66860.49568.6310850.67630.47868.7581860.68400.46198.8851870.69170.44569.0119880.69940.42979.1385890.70720.41419.2649900.71490.39899.3912910.72260.38409.5172920.73030.36949.6430930.73800.35519.7685940.74570.34109.8938950.75340.327310.0188960.76110.313910.1436970.76880.300710.2680980.77650.287810.3922990.78420.275210.51601000.79190.262710.63951010.79960.250610.76271020.80730.238610.88551030.81500.226911.00811040.82280.215411.13021050.83050.204211.25201060.83820.193111.37341070.84590.182211.49441080.85360.171511.61511090.86130.161111.73541100.86900.150811.85521110.87670.140611.97471120.88440.130712.09381130.89210.120912.21241140.89980.111312.33071150.90750.101912.44851160.91520.092612.56591170.92290.083512.68281180.93060.074512.79941190.93830.065712.91551200.94610.057013.03111210.95380.048513.14641220.96150.040113.26111230.96920.031813.37551240.97690.023713.48941250.98460.015713.60281260.99230.007813.71581271.00000.000013.8283"
},




{
"title": "Project Status",
"tags": "project statustimeline",
"keywords": "",
"url": "doc_project_status.html",
"summary": "Here is the project status. It is very much a work in progress as data is being produced at a rapid rate.",
"body": "Halo ProgressTo be added soon..."
},




{
"title": "Rockstar",
"tags": "friends of friendshalo finderRockstarrockstaralgorithmcatalogue",
"keywords": "",
"url": "doc_rockstar.html",
"summary": "The Caterpillar project primarily uses Rockstar for its halo finding. Here you can find an overview of the code and the properties of the halos you might want to access.",
"body": "OverviewRockstar identifies dark matter halos, substructure, and tidal features in phase space.The approach is based on adaptive hierarchical refinement of friends-of-friends groups in six dimensions, which allows for robust (grid-independent, shape-independent, and noise-resilient) tracking of substructure; as such, it is named Rockstar (Robust Overdensity Calculation using K-Space Topologically Adaptive Refinement). The &quot;consistent trees&quot; algorithm for generating merger trees and halo catalogs explicitly ensures consistency of halo properties (mass, position, velocity, radius) across timesteps. The algorithm has demonstrated the ability to increase both the completeness (through inserting otherwise missing halos) and purity (through removing spurious objects) of both merger trees and halo catalogs. In addition, the method is able to robustly measure the self-consistency of halo finders; it is the first to directly measure the uncertainties in halo positions, halo velocities, and the halo mass function for a given halo finder based on actual cosmological simulations.The code&#39;s paper can be found here. Documentation on getting it running and a more exhaustive documentation of the catalogue&#39;s content at its Bitbucket Repository.Rockstar and Iterative UnbindingRockstar is able to find any overdensity in 6D phase space including both halos and streams. To distinguish gravitationally bound halos from other phase space structures, Rockstar performs a single-pass energy calculation to determine which particles are gravitationally bound to the halo. Over-densities where at least 50% ofthe mass is gravitationally bound are considered halos, with the exact fraction a tuneable parameter unbound_threshold of the algorithm Behroozi et al. (2013).This definition is generally very effective at identifying halos and subhalos -- but it fails in two important situations.First, if a subhalo is experiencing significant tidal stripping, the 50$\\%$ cutoff can remove a subhalo from the catalog that should actually exist. We have found that changing the cutoff can recover the missing subhalos, but the best value of the cutoff is not easily determined. Second, Rockstar is occasionally too effective at finding substructure in our high resolution simulations. In particular, it often finds velocity substructures in the cores of our halos that are clearly spurious based on their mass accretion histories and density profiles. Importantly, these two issues do not just affect low mass subhalos, but they can also add or remove halos with \\(V_{max}\\) &gt; 25 km/s.Both of these problems can be alleviated by applying an iterative unbinding procedure. We have implemented such an iterative unbinding procedure within Rockstar. At each iteration, we remove particles whose kinetic energy exceeds the potential energy from other particles in that iteration. The potential is computed with the Rockstar Barnes-Hut method (see Appendix B of Behroozi et al. (2013). We iterate the unbinding until we obtain a self-bound set of particles. Halos are only considered resolved if they contain at least 20 self-bound particles. All halo properties are then computed as usual, but with the self-bound particles instead of the one-pass bound particles. The iterative unbinding recovers the missing subhalos and removes most but not all of the spurious subhalos. Across 13 of our Caterpillar halos, we recover 52 halos with subhalo masses above 108 \\(M_\\odot\\) which would have otherwise been lost using the conventional Rockstar. See Griffen et al. (2015) for further details.Rockstar PropertiesRockstar outputs a large number of halo properties.variableunitsdefinitionnotesidthe ID number of the haloposXMpc/hcomoving z-positionposYMpc/hcomoving y-positionposZMpc/hcomoving z-positionpecVXkm/sPeculiar x-dir physical velocitypecVYkm/sPeculiar y-dir physical velocitypecVZkm/sPeculiar z-dir physical velocitycorevelXkm/sx-dir core velocitycorevelYkm/sy-dir core velocitycorevelZkm/sz-dir core velocitybulkvelXcomoving km/sx-dir bulk velocitybulkvelYcomoving km/sy-dir bulk velocitybulkvelZcomoving km/sz-dir bulk velocitymvirMsol/hvirial mass set by Bryan &amp; Norman prescriptionthis should only be used for the massive host halos - it is best to just use mgravmgravMsol/hvirial mass set by Bryan &amp; Norman prescriptionwhen using the catalogues, only use this quantity as it is the gravitationally bound mass of the halo - especially for subhalosrvirkpc/hvirial radius set by Bryan &amp; Norman prescriptionchild_rvmax_rkm/smgravMsol/hgravitational massvmaxkm/sMaximum circular velocityrvmaxkpc/hradius at maximum circular velocityrskpc/hNFW scale radiusrs_klypinkpc/hKlypin defined scale radiusvrmskm/srms velocityJxx-component angular momentumJyy-component angular momentumJzz-component angular momentumEpotPotential energy of the halospinPeebles defined spin parameterspin_bullockBullock&#39;s modified spin parameterxoffPosition offset parameter abs(center of mass position - halo position)voffVelocity offset parameter abs(center of mass velocity - halo velocity)b_to_aHalo ellipticity b/ac_to_aHalo ellipticity c/ab_to_a2Halo ellipticity b/a (different prescription)c_to_a2Halo ellipticity c/a (different prescription)A[x]x-axis for ellipticity aA[y]y-xis for ellipticity aA[z]z-xis for ellipticity aA2[x]x-axis for ellipticity a (different prescription)A2[y]y-xis for ellipticity a (different prescription)A2[z]z-xis for ellipticity a (different prescription)T/abs(U)Virial ratio (kinetic/abs(potential))min_pos_errMinimum error in positionmin_vel_errMinimum error in bulk velocitymin_bulkvel_errMinimum bulk velocity error.total_npartThe total number of particles associated with this halo (subs included)npartThe number of particles associated with a halo (does not include subhalos)Rockstar Full Particles BinarySummary of changes for full particle binary output. 8/19/2014 (Alex Ji)Alex added a Rockstar option to be able to output in binary all of the particle ids belonging to each halo. This means there is no need to recursively get particles from subhalos in RSDataReader. It also means particles from subhalo fof groups that were deemed unbound or too small, and hence not previously written out, are now correctly written out to the halo they should belong to.Usage: Compiling When compiling Rockstar, use the command % make full_particle_binary This automatically compiles with HDF5 also.Config FileIn the Rockstar config file, specify FULL_PARTICLE_BINARY = num processors to write out full binary files This should be set to the same as NUM_WRITERS. It could be less, but I don&#39;t see any reason for that.Also specify DELETE_BINARY_OUTPUT_AFTER_FINISHED = 1 This deletes the standard .bin files. The new files contain all of the same info as the old .bin files, so they are just a waste of space.Reading DataUse RSDataReader.py version 7. There is now a new parameter called total_npart which specifies the total number of particles belonging to a halo, including substructure. get_particles_from_halo() will now return all particles from a halo. get_all_particles_from_halo() returns exactly the same thing in version=7. The first “npart” particles from any given halo are exactly the same particles you would have gotten in the original Rockstar output. The extra particles after that are the ones we were after.the files now end with the extension .fullbin instead of .binThey are ~10% larger due to the extra particles.Changes to Rockstar codeFiles Modified: config.template.h Makefile halo.h properties.c meta_io.c io_internal.c io_internal.hIn config file, will need to specify FULL_PARTICLE_BINARY = num processors to write out full binary files This config option is now added to config.template.h and by default set to 0.When compiling, specify the option: $ make full_particle_binary defines the flag FULL_PARTICLE_BINARY_FLAG This will compile a version such that the halo struct has an extra parameter total_num_p, found in halo.h. It will also compile with hdf5 since this is the case we most often want. total_num_p is assigned in properties.c in the function calc_additional_halo_props() This corresponds to the total number of particles belonging to the halo, including particles assigned to fof groups that were not printed out (for being unbound, too small, etc.) The extra parameter for each halo needs to be read in with RSDataReader.py version 7. it is named &#39;total_npart&#39; for now.In the file meta_io.c, in the function output_halos() Added another if statement that calls output_full_particles_binary() to print data as we want it. Function called only if FULL_PARTICLE_BINARY &gt; 0 as specified in config file.Wrote output_full_particles_binary() to io_internal.c. It is just like output_binary() except it uses a recursive function, print_child_particles_binary() to print all of the particles, not just num_p of them. Declared output_full_particles_binary() in io_internal.hprint_child_particles_binary() in io_internal.c is my function which is a modification of print_child_particles() to recursively trace child halos and print their particles ids to binary, not ascii. print_child_particles_binary() did not need to be declared in any header file. Just written in file before function that calls it.Output files will now be named halos_x.y.fullbin x is the snap number y is the chunk number. Ranges from 0 to FULL_PARTICLES_BINARY-1In the halo output, p_start corresponds to the starting position in the particle array p of the processor it is on where particles are belonging to that halo. We write this out as numstart in RSDataReader. It was not used before, and now shouldn&#39;t ever be used.At the moment, cat.total_particles will not be truly accurate. It will refer to the sum of npart of every halo. It would be easy to change this to refer to the sum of total_npart for each halo in RSDataReader. I don&#39;t see a good use for cat.total_particles either way though. The binary header information will specify num_particles as the sum of npart, or num_p. Changing this is a bad idea, as it will break other parts of how Rockstar is run. To specify a different particle count, such as number of unique particles written to file, or sum of total_num_p, would require an extra parameter to the struct binary_output_header, or to change num_particles just in the output. I don&#39;t see a good use for this right now.Another note: Files are not written out in order to keep halo ids written out in order. Ex: halos.0.bin might write halos 0 – 250 halos.1.bin would write out halos 1001-1250 and halos.2.bin would write out halos 251-500. This means the unsorted data[i] array in RSDataReader will not always have the property that data[i] is the halo of id = i.Running Rockstar/P-Gadget3This will be a brief tutorial on running Rockstar/P-Gadget3 using code developed by the MIT group (Brendan, Alex, Greg). So you have a Gadget simulation output and you want to run Rockstar on it.First you&#39;ll need a working version of Rockstar which works with the read modules that we will be using later. As of April 2014, the current version of Rockstar (RC2+ with hdf5 support) works with our modules. Be sure to also get consistent-trees-0.9.9.2.Go to your Rockstar-hdf5 directory and run &quot;make with_hdf5&quot;. If it breaks, you need to point to the hdf5 libraries on your system.HDF5_FLAGS = -DH5_USE_16_API -lhdf5 -DENABLE_HDF5 -I/bigbang/data/bgriffen/lib/hdf5-1.8.10/include -L/bigbang/data/bgriffen/lib/hdf5-1.8.10/lib -lhdf5 -lzGo to your consistent-trees directory and run &quot;make all&quot;.Make a folder inside your Rockstar-hdf5/ directory called &quot;cfgs&quot; where you will put your cfg files.The parent simulation configuration file (parent.cfg) for the MIT halos looks like:PARALLEL_IO = 1PARALLEL_IO_SERVER_INTERFACE = &quot;ib0&quot;FORK_READERS_FROM_WRITERS = 1FORK_PROCESSORS_PER_MACHINE = 16NUM_WRITERS = 128INBASE = /bigbang/data/AnnaGroup/caterpillar/parent/gL100X10/outputsFILENAME = snapdir_&lt;snap&gt;/snap_&lt;snap&gt;.&lt;block&gt;.hdf5 NUM_BLOCKS = 256                       FILE_FORMAT = &quot;AREPO&quot;            FULL_PARTICLE_CHUNKS = 1             OUTBASE=/bigbang/data/AnnaGroup/caterpillar/parent/gL100X10/RockstarAREPO_LENGTH_CONVERSION=1AREPO_MASS_CONVERSION=1e+10SNAPSHOT_NAMES=/bigbang/data/AnnaGroup/caterpillar/parent/gL100X10/Rockstar/snapparent.datFORCE_RES=0.002OUTPUT_FORMAT = &quot;BOTH&quot;You&#39;ll have to modify this to suite your needs. This is suited for infiniband using 16 cores per machine and NUM_WRITERS = Nnodes*Ncores being used. NUM_BLOCKS is also for the number of input files for a given snapshot which you specified for Gadget. You can leave it as AREPO as this is just default for using the HDF5 snapshots. You just need to make sure you set the LENGTH and MASS conversions correctly. The ones above are native for P-Gadget3. Be also sure to get FORCE_RES or force softening correct. Here we use 1/40 times the mean inter-particle separation or (1/40)*boxwidth/(np)^(1/3) which in our case is 100/2^10/40 ~ 0.002. If you have the space, I would suggest leaving OUTPUT_FORMAT to be BOTH. You can clean up the files you don&#39;t want later. Snapshot names is just a file listing the snapshot names 000,001,002 ... up to the number of snaps, 127 in this case.You will then have to submit the Rockstar run to the cluster. For a cluster like Stampede or other SLURM build systems you can adapt the following:#!/bin/bash#SBATCH -o parentrock.o%j#SBATCH -e parentrock.e%j#SBATCH -t 48:00:00#SBATCH -p normal#SBATCH --mail-user=bgriffen@mit.edu#SBATCH -J parentrock#SBATCH --mail-type=ALL#SBATCH -n 512module load hdf5rsdir=/home1/02670/bgriffen/Rockstarexe=/home1/02670/bgriffen/Rockstar/rockstarcd $rsdiroutdir=/scratch/02670/bgriffen/gL100X10/Rockstar$exe -c $rsdir/cfgs/parent.cfg &amp;#$exe -c $outdir/restart.cfg &amp; #only use for restarting the run, comment line above.cd $outdirperl -e &#39;sleep 1 while (!(-e &quot;auto-rockstar.cfg&quot;))&#39;srun -n 512 $exe -c auto-rockstar.cfgIf you are running on PBS, please email Brendan Griffen for a PBS version which works just as well."
},




{
"title": "Semi-analytic Model Progress",
"tags": "samsemi-analyticmodellingbaryonsstarscataloguehalos",
"keywords": "",
"url": "doc_sam_progress.html",
"summary": "This is a summary of the semi-analytic models associated with the Caterpillar data set.",
"body": "SAM progressTo be added soon..."
},




{
"title": "Semi-analytic Models",
"tags": "samsmodelsbaryonsstarsstellar",
"keywords": "",
"url": "doc_sams.html",
"summary": "Here is a brief summary of the SAMs which are available on the Caterpillar project.",
"body": "SAMsTo be added soon..."
},




{
"title": "SUBFIND",
"tags": "content-types",
"keywords": "groups, api, structure",
"url": "doc_subfind.html",
"summary": "SUBFIND is our alternative halo finder which we use for comparisons to Rockstar. It is the traditionally SUBFIND found in runs such as Aquarius.",
"body": "SUBFINDFor a nice summary of how SUBFIND works, please see this talk by Volker Springel, the code author.To access the SUBFIND catalogues and associated smoothing lengths for the particles, head to any Caterpillar halo directory on bigbang.mit.edu. There you will find the following:H1387186_EB_Z127_P7_LN7_LX14_O4_NV4 -&gt; outputs/      # gadget raw snapshot output (particle data)  -&gt; groups_319/  # the subfind catalogues are also stored (mostly for the last snapshot)  -&gt; hsmldir_319/ # the smoothing lengths for the corresponding particle data -&gt; analysis/     # post-processed output files (halo profiles, mass functions, minihalos etc.)See Data Access to load the SUBFIND catalogues."
},




{
"title": "Submitting Jobs",
"tags": "slurmpbssubmitjobsqueueclustermitbigbangstampede",
"keywords": "",
"url": "doc_submitting_jobs.html",
"summary": "An introduction to SLURM scripts as well as a few examples for submitting jobs to the cluster.",
"body": "Example SLURM Submission ScriptsScripts can be submitted via using the command sbatch sub.script. There are a number of options you can specify but the key ones are listed here.#SBATCH -n 1This line sets the number of cores that you&#39;re requesting. Make sure that your tool can use multiple cores before requesting more than one.#SBATCH -t 5This line specifies the running time for the job in minutes. If your job runs longer than the value you specify here, it will be cancelled. There is currently no time limit to jobs so you can leave this out for now.#SBATCH -p RegNodesThis line specifies the SLURM partition (AKA queue) under which the script will be run. The RegNodes partition is good for routine jobs that can handle being occasionally stopped and restarted. PENDING times are typically short for this queue as it has the most number of cores.#SBATCH -o hostname.outThis line specifies the file to which standard out will be appended. If a relative file name is used, it will be relative to your current working directory.#SBATCH -e hostname.errThis line specifies the file to which standard error will be appended. SLURM submission and processing errors will also appear in the file.#SBATCH --mail-user=ajk@123.comThe email address to which the --mail-type messages will be sent.#SBATCH --mail-type=ENDBecause jobs are processed in the &quot;background&quot; and can take some time to run, it is useful send an email message when the job has finished (--mail-type=END). Email can also be sent for other processing stages (START, FAIL) or at all of the times (ALL)Here a few example scripts. Contact [mailto:brendan.f.griffen@gmail.com Brendan Griffen] if you would like help constructing something more specific. [https://rc.fas.harvard.edu/resources/running-jobs/ Harvard FAS] has a great number of examples and useful tips.I have a simple serial job which doesn&#39;t require much memory.#!/bin/bash#SBATCH -n 1#SBATCH -o jobname.o%j#SBATCH -e jobname.e%j#SBATCH -p RegNodes#SBATCH -J jobname#SBATCH --share./job.exeI have an openMP task which requires a large amount of memory (up to 256GB).#!/bin/bash#SBATCH -N 1 -n 64#SBATCH -o jobname.o%j#SBATCH -e jobname.e%j#SBATCH -p AMD64#SBATCH -J jobnameexport OMP_NUM_THREADS=64./job.exeI have a MPI job which doesn&#39;t use much memory.#!/bin/bash#SBATCH -N 2 -n 16#SBATCH -o jobname.o%j#SBATCH -e jobname.e%j#SBATCH -p RegNodes#SBATCH -J jobnamempirun -np 16 ./P-Gadget3 param.txt 1&gt;OUTPUT 2&gt;ERRORI have a job which is requires lots of IO between MPI tasks but doesn&#39;t consume much memory.#!/bin/bash#SBATCH -n 16#SBATCH -o jobname.o%j#SBATCH -e jobname.e%j#SBATCH -p HyperNodes#SBATCH -J jobnamempirun -np 16 ./job.exeI have a MPI job which requires a very large amount of memory.#!/bin/bash#SBATCH -n 128#SBATCH -o jobname.o%j#SBATCH -e jobname.e%j#SBATCH -p RegNodes#SBATCH -J jobnamempirun -np 128 ./job.exeVisualizing Cluster UsageThe cluster now can be visualized interactively. Just type &gt; sview in the command prompt on antares.mit.edu and you&#39;ll see the an infographic.Alternatively, you can use Ganglia which can be found here: http://antares.mit.edu/ganglia/SLURM Status &amp; Error HelpStatus MessagesWhen you type squeue -u username you should see a shortened version of one of the following (e.g. PD for PENDING). You can also see these in the sview.PENDING Job is awaiting a slot suitable for the requested resources. Jobs with high resource demands may spend significant time PENDING.RUNNING Job is running.COMPLETEDJob has finished and the command(s) have returned successfully (i.e. exit code 0).CANCELLEDJob has been terminated by the user or administrator using scancel.FAILEDJob finished with an exit code other than 0.Error MessagesJOB &lt;jobid&gt; CANCELLED AT &lt;time&gt; DUE TO TIME LIMITYou did not specify enough time in your batch submission script. The -t option sets time in minutes, or can also take hours:minutes:seconds form (12:30:00 for 12.5 hours)Job &lt;jobid&gt; exceeded &lt;mem&gt; memory limit, being killedYour job is attempting to use more memory than you&#39;ve requested for it. Either increase the amount of memory requested by --mem or --mem-per-cpu or, if possible, reduce the amount your application is trying to use. For example, many Java programs set heap space using the -Xmx JVM option. This could potentially be reduced.slurm_receive_msg: Socket timed out on send/recv operationThis message indicates a failure of the SLURM controller. Though there are many possible explanations, it is generally due to an overwhelming number of jobs being submitted, or, occasionally, finishing simultaneously. If you want to figure out if SLURM is working use the sdiag command. sdiag should respond quickly in these situations and give you an idea as to what the scheduler is up to.JOB &lt;jobid&gt; CANCELLED AT &lt;time&gt; DUE TO NODE FAILUREThis message may arise for a variety of reasons, but it indicates that the host on which your job was running can no longer be contacted by SLURM.Again, see the Harvard FAS SLURM website for extra information and help."
},




{
"title": "Troubleshooting",
"tags": "help",
"keywords": "",
"url": "doc_troubleshooting.html",
"summary": "Here you can find some help!",
"body": "To be added soon."
},




{
"title": "Understanding The Data",
"tags": "dataoutputparticlescataloguesdirectorydirectoriesfolderspaths",
"keywords": "",
"url": "doc_understanding_the_data.html",
"summary": "It is important to understand the layout of Caterpillar data &mdash; particularly if your project involves a lot of minipulation of the particle output.",
"body": "Directory StructureThe master halo directory is here on bigbang.mit.edu: /bigbang/data/AnnaGroup/caterpillar/halos/Every halo has its own directory For example, H1387186 is located here: /bigbang/data/AnnaGroup/caterpillar/halos/H1387186/All production halos follow the format:/bigbang/data/AnnaGroup/caterpillar/halos/H{parent_simulation_id}/Here is the initial list of Caterpillar halos found in Griffen et al. (2015) with their associated ID taken from the Rockstar catalogue of halos for the parent simulation. NamePIDNamePIDNamePIDNamePIDCat-11631506Cat-794687Cat-131725272Cat-191292085Cat-2264569Cat-81130025Cat-141195448Cat-2095289Cat-31725139Cat-91387186Cat-151599988Cat-211232164Cat-444764Cat-10581180Cat-16796175Cat-221422331Cat-55320Cat-111725372Cat-17388476Cat-23196589Cat-6581141Cat-121354437Cat-181079897Cat-241268839Within each of the produdtion runs you will find the following folders.H1387186/ -&gt; H1387186_EB_Z127_P7_LN7_LX12_O4_NV4/ -&gt; H1387186_EB_Z127_P7_LN7_LX14_O4_NV4/ -&gt; H1387186_EB_Z127_P7_LN7_LX11_O4_NV4/   -&gt; H1387186_EB_Z127_P7_LN7_LX13_O4_NV4/ -&gt; H1387186_EB_Z127_P7_LN7_LX14_O4_NV4/ -&gt; contamination_suite/  -&gt; H1195448_BA_Z127_P7_LN7_LX11_O4_NV4    -&gt; H1195448_EA_Z127_P7_LN7_LX11_O4_NV4  -&gt; H1195448_BA_Z127_P7_LN7_LX11_O4_NV5    -&gt; H1195448_EA_Z127_P7_LN7_LX11_O4_NV5  -&gt; H1195448_BB_Z127_P7_LN7_LX11_O4_NV4    -&gt; H1195448_EB_Z127_P7_LN7_LX11_O4_NV4  -&gt; H1195448_BC_Z127_P7_LN7_LX11_O4_NV4    -&gt; H1195448_EC_Z127_P7_LN7_LX11_O4_NV4  -&gt; H1195448_BD_Z127_P7_LN7_LX11_O4_NV4    -&gt; H1195448_EX_Z127_P7_LN7_LX11_O4_NV4  -&gt; H1195448_CA_Z127_P7_LN7_LX11_O4_NV4    -&gt; H1195448_EX_Z127_P7_LN7_LX11_O4_NV5  -&gt; H1195448_CA_Z127_P7_LN7_LX11_O4_NV5The naming convention is as follows: H1387186_EB_Z127_P7_LN7_LX12_O4_NV4  H1387186    # halo rockstar id from parent simulation  EB          # initial conditions type, &#39;B&#39; for box&quot; etc.  Z127        # starting redshift (z = 127)  P7          # padding parameters (2^7)^3  LN7         # level_min used in MUSIC (2^7)^3  LX12        # level_max used in MUSIC (2^12)^3  O4          # overlap parameter (2^4)^3  NV4         # number of times the virial radius enclosed defining lagrangian volumeMost of the terms remain the same for the entire Caterpillar suite. The only ones that do change are the LX value and the B value which signifies the resolution and lagrangian geometry accordingly. NV also represents the number of times the virial radius we enclosed within the parent volume (usually four, though sometimes 5).All halos directories will have three key sub-directories: halos_bound/, outputs/ and analysis/. In each of these directors you&#39;ll find the following folders:H1387186_EB_Z127_P7_LN7_LX14_O4_NV4 -&gt; halos_bound/  # rockstar and merger tree catalogues  -&gt; halos_0/     # each folder contains the catalogue for each snapshot  -&gt; halos_1/  ...  -&gt; halos_319/  -&gt; outputs/  -&gt; trees/    -&gt; forests.list    -&gt; locations.dat    -&gt; tree_0_0_0.dat    -&gt; tree_0_0_1.dat    ...    -&gt; tree_1_1_1.dat    -&gt; tree.bin    -&gt; treeindex.csv -&gt; outputs/      # gadget raw snapshot output (particle data)  -&gt; snapdir_000/ # each folder contains the particle data for each snapshot  -&gt; snapdir_001/  ...  -&gt; snapdir_319/  -&gt; groups_319/  # the subfind catalogues are also stored (mostly for the last snapshot)  -&gt; hsmldir_319/ # the smoothing lengths for the corresponding particle data -&gt; analysis/     # post-processed output files (halo profiles, mass functions, minihalos etc.)You will also notice some other folders, such as halos/ which was the original catalogues produced prior to our implementation of iterative binding. This data can be used to explore the benefits of using full iterative unbinding.Ancillary InformationAt the top level of the halos directory (/bigbang/data/AnnaGroup/caterpillar/halos/), you will find parent_zoom_index.txt. This file contains the following kind of tableparentidictypeLXNVzoomidmin2xyzmvirrvirbadflagbadsubfallsnaps95289BB1147281.12705348.95747.20652.6227.686e+11242.000095289BB12449111.02058448.95347.19352.6257.557e+11240.700195289BB134328950.92403048.96647.19552.6267.607e+11241.200195289BB1442163240.88121148.96247.20552.6257.026e+11234.91111195448EC1142821.68831352.96155.51347.3297.439e+11239.40011195448EC12410421.54453952.95155.51347.3237.572e+11240.80011195448EC13468611.49951352.94455.52647.3087.542e+11240.5001...This above contains Caterpillar as it stands today. The columns are as follows: ColumnDescriptionPARENTIDthe halo id number of the simulation from the parent runICTYPEthe geometry of the initial conditionsLXlevel_max used in MUSICNVnumber of times we enclose the virial radiusMIN2distance to the closest contamination particle (parttype=2)Xx-position in the boxYy-position in the boxZz-position in the boxMVIRhalos mass (in units of Msol/h)RVIRhalo virial radius (in units of kpc/h)BADFLAGa binary flag (1 or 0) if there was a mismatch in parameters between the parent and zoom runBADSUBFa binary flag (1 or 0) if the SUBFIND halo properties do not match the rockstar halo propertiesALLSNAPSa binary flag (1 or 0) if the all the snapshots exist Although it is not recommended, you can load up any of these quantities by importing Brendan Griffen&#39;s modules:import brendanlib.grifflib as glibimport haloutils as htilscatnum = 1lx = 14hpath = htils.get_hpath(catnum,lx)xpos = htils.get_quant_zoom(hpath,&#39;x&#39;)min2 = htils.get_quant_zoom(hpath,&#39;min2&#39;)"
},




{
"title": "XSEDE",
"tags": "single-sourcing",
"keywords": "includes, conref, dita, transclusion, transclude, inclusion, reference",
"url": "doc_xsede.html",
"summary": "Information about the compute clusters at XSEDE.",
"body": "Stampede: the Caterpillar work horseAs per XSEDE website.The TACC Stampede system is a 10 PFLOPS (PF) Dell Linux Cluster based on 6,400+ Dell PowerEdge server nodes, each outfitted with 2 Intel Xeon E5 (Sandy Bridge) processors and an Intel Xeon Phi Coprocessor (MIC Architecture). The aggregate peak performance of the Xeon E5 processors is 2+PF, while the Xeon Phi processors deliver an additional aggregate peak performance of 7+PF. The system also includes a set of login nodes, large-memory nodes, graphics nodes (for both remote visualization and computation), and dual-coprocessor nodes. Additional nodes (not directly accessible to users) provide management and file system services.One of the important design considerations for Stampede was to create a multi-use cyberinfrastructure resource, offering large memory, large data transfer, and GPU capabilities for data-intensive, accelerated or visualization computing. By augmenting some of the compute-intensive nodes within the system with very large memory and GPUs there is no need to move data for data-intensive computing, remote visualization and GPGPU computing. For those situations requiring large-data transfers from other sites, 4 high-speed data servers have been integrated into the Lustre file systems.Compute Nodes: The majority of the 6400 nodes are configured with two Xeon E5-2680 processors and one Intel Xeon Phi SE10P Coprocessor (on a PCIe card). These compute nodes are configured with 32GB of &quot;host&quot; memory with an additional 8GB of memory on the Xeon Phi coprocessor card. A smaller number of compute nodes are configured with two Xeon Phi Coprocessors -- the specific number of nodes available in each configuration at any time will be available from the batch queue summary.Large Memory Nodes: There are an additional 16 large-memory nodes with 32 cores/node and 1TB of memory for data-intense applications requiring disk caching to memory and large-memory methods.Visualization Nodes: For visualization and GPGPU processing 128 compute nodes are augmented with a single NVIDIA K20 GPU on each node with 5GB of on-board GDDR5 memory.File Systems: The Stampede system supports a 14PB global, parallel file storage managed as three Lustre file systems. Each node contains a local 250GB disk. Also, the TACC Ranch tape archival system (60 PB capacity) is accessible from Stampede.Interconnect: Nodes are interconnected with Mellanox FDR InfiniBand technology in a 2-level (cores and leafs) fat-tree topology.Notes about running Caterpillar halos on stampede:You need to specify a minimum 512 cores (-N 32 -n 512) to get started. Depending on the error you should change the following parameters in order and try again.Config.sh: DOUBLEPRECISION_FFTWConfig.sh: MULTIPLEDOMAINS 16 -&gt; 32param.txt.: Buffersize 100 -&gt; 50param.txt: MaxMemSize 3300"
},




{
"title": "Zoom Simulations",
"tags": "zoom simulationsresimulationicsgadgetcontaminationgeometrylagangian",
"keywords": "zoom, simulations,",
"url": "doc_zoomin_simulations.html",
"summary": "Here we provide a brief summary of what was presented in Griffen et al. (2015). We also present some extra details pertaining to the contamination study.",
"body": "The majority of the information about the zoom-in runs can be found in Griffen et al. (2015). Here we simply outline some details which were left out of the publication for the sake of brevity.Overview~Aquarius  LevelMUSIC  levelmaxEffective  Resolution\\(m_{dm}\\)  10^4 h^-1 \\(M_\\odot\\)\\(m_{dm}\\)  10^4 \\(M_\\odot\\)\\(\\epsilon_{dm}\\)  pc11532768^30.250.373621416384^323763138096^316241524124096^31281902285112048^310251527452Each panel represents one single realization of the Cat-1 halo at different resolutions. The far left is an LX11 run and the far right is an LX14 run. Note:  The LX15 run has currently only been run for one of the halos and has been temporarily paused at z = 1. This will be finished with a few others once the main suite has been completed. Warning:  Timesteps are spaced logarithmically in expansion factor to z = 6, then linearly spaced in expansion factor down to z = 0. Always be aware of this as it could be strength and a weakness of your study.We have complete (modified) rockstar halo catalogues (together with consistent-trees merger trees) and z = 0 subfind catalogues.Contamination StudyA number of contamination studies have been carried out. This involves changing the Lagrangian geometry in some way to keep the contamination (distance to the nearest particle type 2 as far as possible) low whilst conserving CPU hours. Our selected test geometries were as followsGeometryDetailBAOriginal MUSIC bounding box (e.g. the exact bounding box of lagr volume).BB1.2 bounding box extentBC1.4 bounding box extentBD1.6 bounding box extentCAConvex Hull VolumeEAOriginal MUSIC ellipsoid (e.g. the exact bounding box of lagr volume).EB1.1 paddingEC1.2 paddingEX1.05 paddingWe did this for both 4 and 5 times the virial radius at z = 0 (marked by the letter 4 or 5 at the end of the abbreviated geometry). Making a total of ~18 test halos per Caterpillar halo. Our requirement was that there was no contamation (particle type 2) within 1 Mpc of the host at the LX11 level.We also looked at how the geometry of the lagrangian volume affected the contamination radius. As outlined in Griffen et al. (2015), we did not find any correlation with geometry and overall level contamination. Every simulation requires its own tailored geometry to achieve our contamination requirements.The size of the lagrangian volumes were also another challenge to overcome. If a halo had LX11 ICs which were larger than 300mb, we found that we could not run these at LX14 on national facilities. The size and distance became our two biggest obstacles when running the Caterpillar suite.Our rockstar catalogues only use the high-resolution particles. This means that there will be halos in the outskirts of the simulation which are contaminated. These are shown clearly below. Be sure not to just take all halos within the rockstar catalogues as some of them will be contaminated (underestimated masses, wrong profiles etc.). As a safety, one should only take halos which are within the contamination distance. This changes as a function of redshift so make sure you update your cut for each snapshot. The plots below are for z = 0."
},




{
"title": "Introduction",
"tags": "getting-started",
"keywords": "",
"url": "index.html",
"summary": "",
"body": "OverviewThis site provides documentation, toolkits, and other notes for the Caterpillar project. There&#39;s a lot of information about how to do a variety of things here, and it&#39;s not all unique to the Caterpillar project. But by and large, understanding how to access the Caterpillar data properly will both speed up your workflow and make assisting you much easier should a problem arise.What You Can Find HereSome of the more prominent features of this wiki include, but are not limited to, the following:general information about the simulation suite and parametersintroduction into the data structure and architecturehundreds of little functions and tools to access the dataexamples of how to manipulate, particle data, halo and merger tree cataloguescompute cluster information for running analysis on either your machine or oursdocumentation about postprocessed datasets such as semi-analytic modelsimportant information about strengths and limitations of the dataset which are not fully publishedrules of thumb to keep in mind (resolution limits, contamination etc.)PDF Download Of DocumentationIf you would like to download this wiki as a PDF, you can do so here. The PDF is mostly the same  as the online help, except that some elements (such as video or special layouts) don&#39;t translate the the print medium, so they&#39;re excluded. PDF DownloadThe PDF contains a timestamp in the header indicating when it was last generated. If you download a PDF, keep in mind that it may go out of date quickly. Always compare your PDF timestamp against the online help timestamp (which you can find in the footer).Questions?If you have questions, contact me at brendan.f.griffen@gmail.com. His regular site is brendangriffen.com. He is eager to make this wiki content as clear as possible, so please let him know if there are areas of confusion that need clarifying."
},













{
"title": "Tag archives overview",
"tags": "navigation",
"keywords": "archives, tagging",
"url": "tag_archives_overview.html",
"summary": "This is an overview to the tag archives section. Really the only reason this section is listed explicitly in the TOC here is to demonstrate how to add a third-level to the navigation.",
"body": "## Reasons for tagsTags provide alternate groupings for your content. In the documentation for this theme, there are a number of equally plausible ways I could have grouped the content. The folder names and items I chose for each item could have been grouped in other ways with good reason.Tags allow you to go beyond the traditional hierarchical classification and provide other groupings. For example, the same item can belong to two different groups. You can also introduce other dimensions not used in your table of contents, such as platform-specific tags or audience-specific tags."
},















null
]


